import os
from pathlib import Path

import numpy as np
import faiss
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from sentence_transformers import SentenceTransformer

# === è¨­å®š ===
# ãƒ­ãƒ¼ã‚«ãƒ«åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
EMBEDDING_MODEL = "intfloat/multilingual-e5-small"
CHAT_MODEL = "gpt-5"  # GPT-5ç³»ãƒ¢ãƒ‡ãƒ«
TOP_K = 3  # ä½•ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã™ã‚‹ã‹

# === OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ– ===
load_dotenv()
try:
    # .env ã® OPENAI_API_KEY ã‚’èª­ã‚€ï¼ˆå›ç­”ç”Ÿæˆç”¨ï¼‰
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
except Exception as e:
    st.error(f"OpenAI APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚: {e}")
    st.stop()

# === ãƒ­ãƒ¼ã‚«ãƒ«åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ– ===
@st.cache_resource(show_spinner="åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
def load_embedding_model():
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    """
    return SentenceTransformer(EMBEDDING_MODEL)


# === ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ ===
def load_documents(data_dir: str = "data"):
    """
    'data'ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰.txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    """
    docs = []
    base = Path(data_dir)
    if not base.exists():
        st.error(f"'{data_dir}' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä½œæˆã—ã¦ãã ã•ã„ã€‚")
        return []

    for p in base.glob("*.txt"):
        try:
            text = p.read_text(encoding="utf-8")
            docs.append(
                {
                    "id": p.name,
                    "path": str(p),
                    "text": text,
                }
            )
        except Exception as e:
            st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ« {p.name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    return docs


# === åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ ===
def embed_texts(texts, model):
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    """
    try:
        embeddings = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
        return embeddings.astype("float32")
    except Exception as e:
        st.error(f"Embeddingã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None


# === ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰ ===
@st.cache_resource(show_spinner="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ã„ã¾ã™...")
def build_index(_embedding_model):
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹
    """
    docs = load_documents()
    if not docs:
        st.error("data/ é…ä¸‹ã« .txt ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚RAGã®æ¤œç´¢å¯¾è±¡ã¨ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    texts = [d["text"] for d in docs]
    embeddings = embed_texts(texts, _embedding_model)

    if embeddings is None:
        st.error("Embeddingã®ç”Ÿæˆã«å¤±æ•—ã—ãŸãŸã‚ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã€‚")
        st.stop()

    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)  # L2è·é›¢ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼‰
    index.add(embeddings)

    return index, embeddings, docs


# === æ¤œç´¢ï¼ˆRetrievalï¼‰ ===
def search_similar_docs(query: str, index, docs, embedding_model, k: int = TOP_K):
    """
    è³ªå•æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€Faissã§é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢ã™ã‚‹
    """
    query_emb = embed_texts([query], embedding_model)  # shape: (1, dim)
    if query_emb is None:
        return []

    distances, indices = index.search(query_emb, k)

    results = []
    for dist, idx in zip(distances[0], indices[0]):
        doc = docs[int(idx)]
        results.append(
            {
                "score": float(dist),
                "doc_id": doc["id"],
                "path": doc["path"],
                "text": doc["text"],
            }
        )
    return results


# === ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦ ===
def build_rag_prompt(question: str, retrieved_docs):
    """
    æ¤œç´¢çµæœã¨è³ªå•æ–‡ã‚’çµ„ã¿åˆã‚ã›ã¦ã€LLMã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹
    """
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ã†ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé•·ã™ãã‚‹ã¨ãã¯é©å½“ã«åˆ‡ã‚‹ï¼‰
    max_chars = 1000
    context_parts = []
    for r in retrieved_docs:
        t = r["text"]
        if len(t) > max_chars:
            t = t[:max_chars] + "\n...(ä»¥ä¸‹ç•¥)"
        context_parts.append(f"[{r['doc_id']}]\n{t}")

    context = "\n\n---\n\n".join(context_parts)

    system_prompt = (
        "ã‚ãªãŸã¯ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ›¸ã‹ã‚Œã¦ã„ãªã„ã“ã¨ã¯æ¨æ¸¬ã›ãšã€ã€Œåˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚"
    )

    user_prompt = f"""ä»¥ä¸‹ã¯ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡ºã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{context}

---
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•
{question}

---
ä¸Šè¨˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã ã‘ã‚’æ ¹æ‹ ã«ã€æ—¥æœ¬èªã§ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ååˆ†ãªæƒ…å ±ãŒãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ­£ç›´ã«ä¼ãˆã¦ãã ã•ã„ã€‚
"""

    return system_prompt, user_prompt


# === å›ç­”ç”Ÿæˆï¼ˆGenerationï¼‰ ===
def generate_answer(system_prompt: str, user_prompt: str):
    """
    OpenAI Responses APIã‚’å©ã„ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹
    """
    try:
        resp = client.responses.create(
            model=CHAT_MODEL,
            instructions=system_prompt,
            input=user_prompt,
            # GPT-5ã¯ temperature å›ºå®šãªã®ã§æŒ‡å®šã—ãªã„
        )
        # ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒä¸€ã¤ã«ã¾ã¨ã¾ã£ãŸãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
        return resp.output_text
    except Exception as e:
        st.error(f"OpenAI APIã®å‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None


# === Streamlit UI ===
def main():
    st.set_page_config(page_title="RAGã‚’ã‚¼ãƒ­ã‹ã‚‰å®Ÿè£…ã™ã‚‹ã€2025å¹´ç‰ˆã€‘", layout="wide")
    st.title("RAGã‚’ã‚¼ãƒ­ã‹ã‚‰å®Ÿè£…ã—ã¦ä»•çµ„ã¿ã‚’å­¦ã¶ã€2025å¹´ç‰ˆã€‘")
    st.caption(f"ãƒ¢ãƒ‡ãƒ«: {CHAT_MODEL} | Embedding: {EMBEDDING_MODEL} (ãƒ­ãƒ¼ã‚«ãƒ«) | æ¤œç´¢: Faiss")

    st.write(
        f"ã“ã®ãƒ‡ãƒ¢ã§ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ã® `{Path('data').resolve()}` ãƒ•ã‚©ãƒ«ãƒ€å†…ã«ã‚ã‚‹ `.txt` ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢å¯¾è±¡ã«ã—ãŸã€ã‚·ãƒ³ãƒ—ãƒ«ãªRAGã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚"
    )

    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    try:
        embedding_model = load_embedding_model()
    except Exception as e:
        st.error(f"åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚: {e}")
        st.stop()

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    try:
        index, embeddings, docs = build_index(embedding_model)
    except Exception as e:
        st.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚: {e}")
        st.stop()

    with st.sidebar:
        st.header("è¨­å®š")
        top_k = st.slider("å‚ç…§ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°", 1, 10, TOP_K)

        st.markdown("### èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§")
        if docs:
            for d in docs:
                st.markdown(f"- `{d['id']}`")
        else:
            st.warning("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    question = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šãƒŠãƒ¬ãƒƒã‚¸ã‚»ãƒ³ã‚¹ã¨ã¯ï¼Ÿï¼‰")
    run = st.button("RAGã«èã„ã¦ã¿ã‚‹")

    if run and question:
        with st.spinner("æ¤œç´¢ & å›ç­”ç”Ÿæˆä¸­..."):
            # 1. æ¤œç´¢
            retrieved = search_similar_docs(question, index, docs, embedding_model, k=top_k)

            if not retrieved:
                st.error("æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                st.stop()

            # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            system_prompt, user_prompt = build_rag_prompt(question, retrieved)

            # 3. å›ç­”ç”Ÿæˆ
            answer = generate_answer(system_prompt, user_prompt)

        if answer:
            col1, col2 = st.columns(2)

            with col1:
                st.subheader("ğŸ¤– å›ç­”")
                st.write(answer)

            with col2:
                st.subheader("ğŸ“š æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ")
                for r in retrieved:
                    with st.expander(f"`{r['doc_id']}`ï¼ˆé¡ä¼¼ã‚¹ã‚³ã‚¢: {r['score']:.4f}ï¼‰"):
                        st.text(r["text"][:1500])  # è¡¨ç¤ºã—ã™ãã‚‹ã¨é‡ã„ã®ã§é©å½“ã«åˆ‡ã‚‹
        else:
            st.error("å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    elif run and not question:
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
